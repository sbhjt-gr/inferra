# Inferra

Inferra is a mobile application built on React Native and Expo that brings large language models (LLMs) & small language models (SLMs) directly to your Android & iOS device. Cloud-based models (remote models) like Claude, DeepSeek, Gemini and ChatGPT are also supported. File attachments are also well-supported for both local & remote models.

<a href="https://play.google.com/store/apps/details?id=com.gorai.ragionare">
  <img src="https://github.com/user-attachments/assets/bdc18fc5-5a99-410c-b383-eaf9c737176e" alt="play_download" width="200"/>
</a>
<br>
<i>iOS version is ready and scheduled to be released soon.</i>
<br><br>
If you want to support me and the development of this project, kindly donate through <a href="https://ko-fi.com/subhajitgorai" target="_blank">Ko-fi</a>. Any donation amount is highly appreciated.

<br>

## Screenshots
<img src="https://github.com/user-attachments/assets/28e9720f-1e3c-460d-b189-7f31d5020a90" alt="play_download" />

## Features

- Inference, currently only through llama.cpp, is supported but I plan to add more inference engines as I keep working on this project. You can become a contributor to this project by implementing some of these features. See the [contributions guide](#contributing) below.
- Seamless integration with cloud-based models from OpenAI, Gemini, Anthropic and DeepSeek is also supported, but you need to have your own API keys and an Inferra registered account for that. Using remote models is completely optional.
- The app supports vision through models with multimodal capabilities. First, you need a multimodal model and its corresponding multimodal projector (mmproj) file which you can find <a target="_blank" href="https://github.com/ggml-org/llama.cpp">here</a>. I've included SmolVLM2 and its corresponding multimodal projector (mmproj) file by default which you can find in the Models -> Download Models tab. Both are combined, meaning downloading "SmolVLM2" will also download its associated projector file, but you can cancel the download of the other file if you don't want it.
- The app comes with a built-in camera (based on expo-camera), which lets you click pictures in the app itself and send them to the model directly. The clicked pictures are saved to your gallery by default. 
- The file attachment support comes with a built-in document extractor which will perform OCR locally on all the pages of your document and extract the text content and send it to the model (whether it's local/remote). I'm working on a RAG-based approach to implement this, which will make the processing much faster. You can help me implement this feature by [contributing](#contributing)!
- The app comes with a download manager which directly downloads models from HuggingFace. I've cherry-picked many models and made a list ideal for running models on Edge devices. You can find them by going to the Models -> Download Models tab. Any models you download will be visible in the model selector of the chat screen as well as in the "Stored Models" tab of the "Models" tab. You can import models from your local storage as well.
- Messages sent to the model have editing, regeneration, copy functionality and markdown support. Any code generated by the model is rendered inside a codeblock and has clipboard copying functionality.

## Architecture

### Project Structure
This is the rough file structure of the project. This may not be up-to-date as the file structure is constantly evolving.
```
src/
├── components/          # Reusable UI components
│   ├── chat/           # Chat interface components
│   ├── model/          # Model management components
│   └── settings/       # Settings screen components
├── constants/          # App constants, themes, and configurations
├── context/            # React context providers
│   ├── DialogContext.tsx      # Dialog management
│   ├── DownloadContext.tsx    # Download state management
│   ├── ModelContext.tsx       # Local model management
│   ├── RemoteModelContext.tsx # Cloud model management
│   └── ThemeContext.tsx       # Theme and appearance
├── navigation/         # Navigation configuration
├── screens/           # Main application screens
│   ├── HomeScreen.tsx         # Main chat interface
│   ├── ModelScreen.tsx        # Model management
│   ├── SettingsScreen.tsx     # App configuration
│   ├── ChatHistoryScreen.tsx  # Conversation history
│   ├── DownloadsScreen.tsx    # Download management
│   └── ProfileScreen.tsx      # User profile
├── services/          # Core business logic
│   ├── Authentication/        # Auth services (Firebase, Google, Email)
│   ├── AI Services/          # AI provider integrations
│   ├── ModelDownloader.ts    # Local model management
│   ├── FileManager.ts        # File operations
│   └── NotificationService.ts # Push notifications
├── types/             # TypeScript type definitions
└── utils/            # Utility functions and managers
    ├── ChatManager.ts        # Chat state management
    ├── LlamaManager.ts       # Local LLM interface
    ├── PDFOcrUtils.ts        # Document processing
    └── ImageProcessingUtils.ts # Image handling
```

## Tech Stack

- **React Native + Expo**: For cross-platform support.
- **TypeScript**: The syntactical superset of JavaScript, widely used for React Development.
- **Firebase**: For authentication, Firestore database, and cloud services.
- **inferra-llama**: Custom llama.cpp bridge for local inference originally maintained by <a href="https://www.bricks.tools/" target="_blank">BRICS</a>.
- **React Navigation**: For navigation and routing
- **React Native Paper**: Used for many Material Design UI components, although the whole UI is not purely based on the Material design.
- **React Native ML Kit**: For on-device text recognition and OCR
- **ESLint**: For code quality
- **Some Expo Modules**: For camera, file system, notifications, device APIs etc.

## Getting Started
If you want to contribute or just try to run it locally, follow the guide below. Please adhere to the rules of the <a href="https://github.com/sbhjt-gr/inferra/blob/main/LICENSE">LICENSE</a> because you are not supposed to just `git clone` and pass it as your own work.

### Prerequisites

- Node.js (>= 16.0.0, < 23.0.0)
- npm or yarn
- Expo CLI
- Android Studio (for Android development)
- Xcode (for iOS development)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/sbhjt-gr/inferra
   cd inferra
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Configure your API keys and Firebase settings as shown in the example env.
   ```

4. **Run on device or emulator**
   ```bash
   # For Android
   npx expo run:android
   
   # For iOS
   npx expo run:ios
   ```

## License

This project is distributed under the Apache License 2.0 License. Please read it <a href="https://github.com/sbhjt-gr/inferra/blob/main/LICENSE">here</a>. Any modifications must adhere to the rules of this LICENSE.

## Contributing

Contributions are welcome! You can find reported bugs in the <a href="https://github.com/sbhjt-gr/inferra/issues">issues</a> tab. Comment down on the issue if you are willing to work on that particular issue. You should be assigned to the task first before you start working. If you want to contribute a feature of your own, open a new issue first and describe your idea clearly:
Explain:
  - What the feature is
  - Why it's useful
  - How you plan to implement it

After you've been assigned to it, you can start working on it.

Some features that I'm keen to implement are:

- Support for different inference engines other than llama.cpp for including MNN & MLX (Apple-specific) which require custom native code.
- RAG-based implementation for file attachments. 

If you're interested to work on any one of these (or any other idea you have), you are welcome to open an issue.

## Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - This the default underlying engine for running local LLMs and it's the only one that's been implemented yet.
- [inferra-llama.rn](https://github.com/sbhjt-gr/inferra-llama.rn) - This is the customized React Native adapter which provides the bridge for llama.cpp. Originally forked and self-hosted from <a href="https://github.com/mybigday/llama.rn" target="_blank">llama.rn</a> for updating llama.cpp more frequently.
- If someone thinks they also need to be mentioned here, please let me know.
